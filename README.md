# End-to-End Data Engineering Pipeline with Apache Airflow, Spark & Docker

This project is a hands-on implementation of a full data engineering pipeline combining **Apache Airflow**, **Apache Spark**, **Docker**, and multiple programming languages (**Python**, **Scala**, and **Java**). It follows a guided course and simulates real-world data workflows â€” from cluster setup to job submission and result tracking.

---

### Project Overview

I designed and deployed an end-to-end data engineering environment that includes:

-  **Airflow & Spark Clusters** orchestrated in Docker
-  **Python-based Spark Jobs** for simple data processing
-  **Scala & Java Spark Jobs** to handle additional use cases
-  **Live submission of Spark jobs** to the cluster and real-time result tracking

This project highlights my ability to integrate diverse technologies and manage complex workflows in a scalable, production-like environment.

---

###  What I Built

-  Set up **Spark and Airflow clusters** using **Docker Compose**
-  Wrote **Python Spark jobs** for initial ETL transformations
-  Developed **Scala Spark jobs**, compiled and executed them in the cluster
-  Built and submitted **Java-based Spark jobs** to the cluster
-  Managed orchestration and scheduling with **Apache Airflow**
-  Monitored Spark job execution and observed **live results** across the cluster

---

###  Tech Stack

- **Apache Airflow** â€“ Workflow orchestration
- **Apache Spark** â€“ Distributed data processing
- **Docker & Docker Compose** â€“ Containerization of cluster components
- **Python, Scala, Java** â€“ Spark job development
- **JAR & SBT** â€“ For compiling Scala and Java jobs
- **Bash scripting & CLI** â€“ For cluster control and job submissions

---

###  Key Highlights

-  Successfully ran distributed jobs written in **three programming languages**
-  Orchestrated workflows using Airflowâ€™s DAG structure
-  Gained practical knowledge in managing **multi-language Spark workloads**
-  Built a fully operational local **data engineering pipeline environment**

---

### ðŸ“˜ Learning Outcome

This project allowed me to deepen my understanding of:

- Airflow DAGs and task dependencies
- Spark's processing model across different languages
- Managing containerized big data environments
- End-to-end data engineering architecture, from orchestration to computation

---


---

### ðŸ“š Source Course

This project is based on the excellent tutorial by Data Mastery Lab. You can explore more of their data engineering content here:  
ðŸ”— [https://datamasterylab.com/sign_up](https://datamasterylab.com/sign_up)

---

ðŸ“Œ _Feel free to fork or star this project if you're exploring similar setups or need a starter template!_
